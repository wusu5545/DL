{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### V1：Simple Self-Attention Layer\n",
    "\n",
    "This is a simple implementation of a self-attention layer. It takes a sequence of vectors as input and produces a sequence of the same length, where each vector is a weighted sum of the input vectors.\n",
    "\n",
    "The self-attention mechanism allows the model to focus on different parts of the input sequence when making predictions, which can be particularly useful for tasks such as language modeling and translation."
   ],
   "id": "8635c95aee08a12b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T04:06:31.785824Z",
     "start_time": "2024-10-31T04:06:30.265835Z"
    }
   },
   "source": [
    "###simple version 1\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.query = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.key = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.value = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        #X shape is : (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.value(X)\n",
    "        # Q K V shape (batch, seq , hidden_dim)\n",
    "        \n",
    "        # attention_value is : (batch, seq, seq)\n",
    "        attention_value = torch.matmul(\n",
    "            # K.T shape : (batch, hidden_dim, seq)\n",
    "            Q, K.transpose(-1,-2)\n",
    "        )\n",
    "        \n",
    "        # attention_weight shape : (batch, seq, seq)\n",
    "        #divide by sqrt(hidden_dim) to prevent gradient vanish\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim),\n",
    "            dim=-1 #softmax at last dim\n",
    "        )\n",
    "        print(attention_weight)\n",
    "        \n",
    "        # (batch, seq ,hidden_dim)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "self_attention_net = SelfAttentionV1(4)\n",
    "self_attention_net(X)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5332, 0.4668],\n",
      "         [0.5271, 0.4729]],\n",
      "\n",
      "        [[0.4961, 0.5039],\n",
      "         [0.4906, 0.5094]],\n",
      "\n",
      "        [[0.5510, 0.4490],\n",
      "         [0.5345, 0.4655]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4533, -0.0090,  0.1798,  0.5238],\n",
       "         [-0.4566, -0.0082,  0.1788,  0.5268]],\n",
       "\n",
       "        [[-0.2665, -0.2063,  0.0037,  0.5727],\n",
       "         [-0.2649, -0.2069,  0.0046,  0.5712]],\n",
       "\n",
       "        [[-0.5426, -0.3203, -0.2754,  0.7483],\n",
       "         [-0.5384, -0.3169, -0.2683,  0.7455]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### V2：Efficiency Optimization",
   "id": "1b89db9a32b5cc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T04:06:53.718192Z",
     "start_time": "2024-10-31T04:06:53.698854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim: int = 728) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # X shape (batch, seq, dim)\n",
    "        # QKV shape (batch, seq, dim * 3)\n",
    "        QKV = self.qkv(X)\n",
    "        \n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "\n",
    "        attention_weight = torch.softmax(\n",
    "            torch.matmul(Q, K.transpose(-1,-2)\n",
    "            ) / math.sqrt(self.dim), \n",
    "            dim=-1\n",
    "        )\n",
    "        print(attention_weight)\n",
    "        output = attention_weight @ V\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "self_attention_net = SelfAttentionV2(4)\n",
    "self_attention_net(X)"
   ],
   "id": "864020a0c54bbfab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5095, 0.4905],\n",
      "         [0.5166, 0.4834]],\n",
      "\n",
      "        [[0.5135, 0.4865],\n",
      "         [0.5115, 0.4885]],\n",
      "\n",
      "        [[0.4994, 0.5006],\n",
      "         [0.4932, 0.5068]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2236, -0.0855,  0.4374,  0.8119],\n",
       "         [-0.2232, -0.0832,  0.4369,  0.8130]],\n",
       "\n",
       "        [[-0.1544, -0.3866,  0.5923,  0.9547],\n",
       "         [-0.1546, -0.3868,  0.5921,  0.9546]],\n",
       "\n",
       "        [[-0.1830, -0.0566,  0.3942,  0.9942],\n",
       "         [-0.1830, -0.0578,  0.3947,  0.9941]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### V3: Add Some Details",
   "id": "d31216dca832e73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T04:06:56.762838Z",
     "start_time": "2024-10-31T04:06:56.730352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.dropout position\n",
    "# 2.attention_mask\n",
    "# 3.output projection\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout_rate\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output = nn.Linear(dim, dim)\n",
    "       \n",
    "    def forward(self, X: torch.Tensor, attention_mask=None) -> torch.Tensor:\n",
    "        # X (batch, seq, dim)\n",
    "        QKV = self.qkv(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        \n",
    "        attention_weight = Q @ K.transpose(-1,-2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        # (batch, seq ,seq)\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight, \n",
    "            dim=-1\n",
    "        )\n",
    "        print(attention_weight)\n",
    "        \n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        output = self.output(attention_weight @ V)\n",
    "        # (batch, seq, dim)\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 4, 2)\n",
    "# (batch, seq, seq) (batch, seq)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0], \n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(mask.shape)\n",
    "#Expand the mask tensor to match the shape of the input tensor for broadcasting compatibility\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(f\"repeat shape is: {mask.size()}\")\n",
    "\n",
    "attention_net = SelfAttentionV3(2)\n",
    "attention_net(X, mask)"
   ],
   "id": "23c3265b8e76f4c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "repeat shape is: torch.Size([3, 4, 4])\n",
      "tensor([[[0.3685, 0.3538, 0.2777, 0.0000],\n",
      "         [0.3601, 0.3496, 0.2903, 0.0000],\n",
      "         [0.3374, 0.3346, 0.3280, 0.0000],\n",
      "         [0.3736, 0.3562, 0.2702, 0.0000]],\n",
      "\n",
      "        [[0.4933, 0.5067, 0.0000, 0.0000],\n",
      "         [0.4467, 0.5533, 0.0000, 0.0000],\n",
      "         [0.4558, 0.5442, 0.0000, 0.0000],\n",
      "         [0.4586, 0.5414, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2232, -0.7291],\n",
       "         [-0.0272, -0.7461],\n",
       "         [-0.0371, -0.7525],\n",
       "         [-0.0219, -0.7426]],\n",
       "\n",
       "        [[-0.3550, -0.7401],\n",
       "         [-0.1194, -0.7139],\n",
       "         [-0.1213, -0.7164],\n",
       "         [-0.3631, -0.7319]],\n",
       "\n",
       "        [[ 0.1005, -0.6753],\n",
       "         [ 0.1005, -0.6753],\n",
       "         [ 0.1005, -0.6753],\n",
       "         [ 0.1005, -0.6753]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### V4: Multi-Head Self-Attention",
   "id": "f176e28620d36758"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T04:07:01.130733Z",
     "start_time": "2024-10-31T04:07:01.100948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads # (num_heads * head_dim = dim)\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3) # (dim, head_dim * num_heads * 3)\n",
    "        self.output = nn.Linear(in_features=dim, out_features=dim)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor, attention_mask=None) -> torch.Tensor:\n",
    "        # X (b, s, h)\n",
    "\n",
    "        batch, seq, _ = X.size()\n",
    "        \n",
    "        QKV = self.qkv(X)\n",
    "        # (b, s, h)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        # (b, s, h) => (b, num_heads, s, head_dim)\n",
    "        # h => num_heads * head_dim\n",
    "        q_state = Q.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # (b, num_heads, s, s)\n",
    "        # (b, num_heads, s, head_dim) => (b, num_heads, head_dim, s)\n",
    "        attention_weight = q_state @ k_state.transpose(-1,2) / math.sqrt(self.dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attention_weight = torch.softmax(attention_weight, dim=-1)\n",
    "        print(attention_weight.shape)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        output_mid = attention_weight @ v_state # (b, num_heads, s, head_dim)\n",
    "        \n",
    "        # (b, s, h)\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output_mid = output_mid.view(batch, seq, -1) # h\n",
    "        output = self.output(output_mid)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [0, 0],\n",
    "            [1, 0]\n",
    "        ]\n",
    "    )\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .expand(3, 8, 2, 2)\n",
    ")\n",
    "\n",
    "X = torch.rand(3, 2, 128)\n",
    "attention_net = MultiHeadAttention(dim=128, num_heads=8) # head_dim = 16\n",
    "attention_net(X, mask)"
   ],
   "id": "e0c7fd4bfeba916f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0997, -0.1123, -0.1628, -0.0790, -0.0662,  0.2278, -0.4019,\n",
       "          -0.1423,  0.1320,  0.0509,  0.0600, -0.1963, -0.4512, -0.3134,\n",
       "           0.1748, -0.0897,  0.0406, -0.1287, -0.2564,  0.0798,  0.2863,\n",
       "          -0.0454, -0.2507,  0.0787, -0.5036,  0.1970, -0.0495,  0.2191,\n",
       "           0.0671, -0.0168,  0.0675, -0.4743, -0.2189,  0.3280, -0.1422,\n",
       "          -0.3457, -0.1562,  0.4148,  0.3013, -0.1683, -0.0898,  0.0133,\n",
       "          -0.2101, -0.1575,  0.4553, -0.0332, -0.4689,  0.3274, -0.2142,\n",
       "          -0.2233, -0.4041,  0.0894, -0.1285,  0.6013,  0.0090,  0.0839,\n",
       "           0.2653, -0.0127,  0.2401, -0.0509, -0.0785,  0.1627,  0.1882,\n",
       "          -0.0582, -0.0187, -0.0171,  0.0437,  0.0630,  0.2385, -0.1078,\n",
       "           0.0395,  0.0492,  0.2838,  0.0390, -0.2167, -0.2571, -0.0387,\n",
       "          -0.1593, -0.1299,  0.2698, -0.1954,  0.5995, -0.5483, -0.3328,\n",
       "          -0.0645,  0.0079,  0.1874, -0.1107, -0.3210,  0.1329, -0.1958,\n",
       "          -0.1351, -0.3772, -0.0134, -0.1852, -0.1042, -0.1738,  0.1783,\n",
       "          -0.4962, -0.1456, -0.0510,  0.0362, -0.1796, -0.0756, -0.0212,\n",
       "           0.2038,  0.1157, -0.0379, -0.1898,  0.1884,  0.4555,  0.1376,\n",
       "          -0.1357,  0.1166, -0.1754, -0.1092,  0.0819,  0.3155, -0.0318,\n",
       "          -0.0617, -0.4124,  0.5227,  0.3007, -0.2872,  0.0939, -0.0579,\n",
       "          -0.0687,  0.0903],\n",
       "         [ 0.0988, -0.1415, -0.0634,  0.0855, -0.1717,  0.1458, -0.3540,\n",
       "          -0.2250,  0.0792,  0.1559, -0.0761, -0.2172, -0.2590, -0.1708,\n",
       "           0.1235, -0.0556,  0.0544, -0.1286, -0.2672,  0.1064,  0.1947,\n",
       "          -0.0510, -0.1537,  0.1645, -0.3576,  0.2111, -0.0204,  0.0693,\n",
       "           0.0282, -0.0693,  0.0144, -0.3402, -0.1537,  0.2447, -0.0625,\n",
       "          -0.2391, -0.0778,  0.3383,  0.4345, -0.0785,  0.0232, -0.0681,\n",
       "          -0.1271, -0.1012,  0.3828,  0.0603, -0.3159,  0.3644, -0.1083,\n",
       "          -0.1682, -0.2869,  0.0874, -0.2736,  0.4197, -0.1220,  0.0484,\n",
       "           0.2193,  0.0494,  0.1950, -0.0144,  0.0150,  0.0309,  0.1142,\n",
       "          -0.1731, -0.1461, -0.0881,  0.0354, -0.0111, -0.0172, -0.0530,\n",
       "           0.1250,  0.0925,  0.2654,  0.0366, -0.1284, -0.2404,  0.0922,\n",
       "          -0.0358, -0.2940,  0.2941, -0.0653,  0.2896, -0.3741, -0.1889,\n",
       "           0.0822, -0.0612,  0.0531, -0.1563, -0.2947,  0.2375, -0.1912,\n",
       "          -0.1349, -0.2959, -0.0110, -0.0397, -0.0591, -0.0870,  0.0762,\n",
       "          -0.3913, -0.1199,  0.0474, -0.0066, -0.1102, -0.1477, -0.0159,\n",
       "           0.1586,  0.1717, -0.0336, -0.1189,  0.1828,  0.3275,  0.0573,\n",
       "          -0.1091,  0.0467, -0.0355,  0.0876,  0.0453,  0.1228, -0.0116,\n",
       "          -0.0628, -0.1333,  0.4508,  0.3010, -0.2615,  0.0332,  0.0289,\n",
       "          -0.1492, -0.0150]],\n",
       "\n",
       "        [[    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan]],\n",
       "\n",
       "        [[ 0.2341, -0.3121, -0.3866, -0.3187, -0.0989, -0.1684, -0.2829,\n",
       "           0.0191,  0.4299,  0.2473,  0.2689, -0.2057, -0.1679, -0.1467,\n",
       "           0.0730, -0.0465, -0.1337, -0.1475, -0.0494, -0.1545,  0.1539,\n",
       "          -0.0940, -0.2393, -0.0806, -0.6199,  0.1059, -0.0344,  0.0419,\n",
       "           0.3192,  0.1587,  0.2083, -0.1580, -0.1677,  0.0523, -0.0871,\n",
       "          -0.3190,  0.1863,  0.2593,  0.1033, -0.3408, -0.0097,  0.0837,\n",
       "          -0.2808, -0.1483,  0.0320,  0.0702, -0.4812,  0.2800, -0.2976,\n",
       "          -0.4471, -0.7427,  0.0132, -0.1204,  0.4268,  0.0505,  0.0493,\n",
       "           0.2067, -0.0519,  0.1887, -0.0014, -0.1128, -0.1098,  0.1313,\n",
       "          -0.0342,  0.0853, -0.1331,  0.0716, -0.0107,  0.3076, -0.1206,\n",
       "           0.0437,  0.2915,  0.2363,  0.1873, -0.1850, -0.1716, -0.0759,\n",
       "          -0.2129, -0.0782,  0.3560, -0.1235,  0.3864, -0.7464, -0.4177,\n",
       "           0.1766, -0.0909,  0.2610, -0.1351, -0.4329,  0.1012, -0.0759,\n",
       "           0.0589, -0.2013, -0.1014, -0.3816, -0.0534, -0.1314,  0.3329,\n",
       "          -0.1916,  0.0330,  0.0519,  0.1430, -0.1440, -0.0628, -0.0157,\n",
       "           0.1716, -0.0996, -0.1181, -0.0990,  0.3277,  0.2808,  0.1430,\n",
       "          -0.0381,  0.0676, -0.3358,  0.0043,  0.0227,  0.4437, -0.0366,\n",
       "          -0.1882, -0.5690,  0.3531,  0.0426, -0.0980, -0.0164,  0.0782,\n",
       "           0.0585,  0.1377],\n",
       "         [ 0.2341, -0.3121, -0.3866, -0.3187, -0.0989, -0.1684, -0.2829,\n",
       "           0.0191,  0.4299,  0.2473,  0.2689, -0.2057, -0.1679, -0.1467,\n",
       "           0.0730, -0.0465, -0.1337, -0.1475, -0.0494, -0.1545,  0.1539,\n",
       "          -0.0940, -0.2393, -0.0806, -0.6199,  0.1059, -0.0344,  0.0419,\n",
       "           0.3192,  0.1587,  0.2083, -0.1580, -0.1677,  0.0523, -0.0871,\n",
       "          -0.3190,  0.1863,  0.2593,  0.1033, -0.3408, -0.0097,  0.0837,\n",
       "          -0.2808, -0.1483,  0.0320,  0.0702, -0.4812,  0.2800, -0.2976,\n",
       "          -0.4471, -0.7427,  0.0132, -0.1204,  0.4268,  0.0505,  0.0493,\n",
       "           0.2067, -0.0519,  0.1887, -0.0014, -0.1128, -0.1098,  0.1313,\n",
       "          -0.0342,  0.0853, -0.1331,  0.0716, -0.0107,  0.3076, -0.1206,\n",
       "           0.0437,  0.2915,  0.2363,  0.1873, -0.1850, -0.1716, -0.0759,\n",
       "          -0.2129, -0.0782,  0.3560, -0.1235,  0.3864, -0.7464, -0.4177,\n",
       "           0.1766, -0.0909,  0.2610, -0.1351, -0.4329,  0.1012, -0.0759,\n",
       "           0.0589, -0.2013, -0.1014, -0.3816, -0.0534, -0.1314,  0.3329,\n",
       "          -0.1916,  0.0330,  0.0519,  0.1430, -0.1440, -0.0628, -0.0157,\n",
       "           0.1716, -0.0996, -0.1181, -0.0990,  0.3277,  0.2808,  0.1430,\n",
       "          -0.0381,  0.0676, -0.3358,  0.0043,  0.0227,  0.4437, -0.0366,\n",
       "          -0.1882, -0.5690,  0.3531,  0.0426, -0.0980, -0.0164,  0.0782,\n",
       "           0.0585,  0.1377]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4273dcbe512fce6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
